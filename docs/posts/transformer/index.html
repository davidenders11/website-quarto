<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="David Enders">
<meta name="dcterms.date" content="2023-10-24">

<title>David Enders - The Game of AI: And How the Transformer Transformed It</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">David Enders</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/davidenders11" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/in/david-enders" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://medium.com/@davidenders" rel="" target=""><i class="bi bi-medium" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:david.georg.enders@gmail.com" rel="" target=""><i class="bi bi-envelope" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../resume.pdf" rel="" target="">
 <span class="menu-text">Resume</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html" rel="" target="">
 <span class="menu-text">My Writing</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-turing-test-how-it-started" id="toc-the-turing-test-how-it-started" class="nav-link active" data-scroll-target="#the-turing-test-how-it-started">The Turing Test: How it Started</a></li>
  <li><a href="#notable-following-accomplishments-in-ai" id="toc-notable-following-accomplishments-in-ai" class="nav-link" data-scroll-target="#notable-following-accomplishments-in-ai">Notable Following Accomplishments in AI</a></li>
  <li><a href="#transformers-the-structure-and-implementation" id="toc-transformers-the-structure-and-implementation" class="nav-link" data-scroll-target="#transformers-the-structure-and-implementation">Transformers: The Structure and Implementation</a></li>
  <li><a href="#impact-and-applications" id="toc-impact-and-applications" class="nav-link" data-scroll-target="#impact-and-applications">Impact and Applications</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">The Game of AI: And How the Transformer Transformed It</h1>
  <div class="quarto-categories">
    <div class="quarto-category">computer science</div>
    <div class="quarto-category">neural networks</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>David Enders </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 24, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>For decades, the world has been fascinated with the concept of artificial intelligence; we have written stories about it, produced movies, written countless computer programs, and theorized about what is possible to achieve and what isn’t. This history is an impressive one, but also somewhat humorous to me; it all seems like a bit of a game, with teams of scientists and researchers competing to create a machine that can play chess the best, or draw the best pictures, or write more human-like stories.</p>
<p><img src="media/media/image1.png" class="img-fluid"></p>
<section id="the-turing-test-how-it-started" class="level2">
<h2 class="anchored" data-anchor-id="the-turing-test-how-it-started">The Turing Test: How it Started</h2>
<p>In 1950, Alan Turing published a paper titled <a href="https://academic.oup.com/mind/article/LIX/236/433/986238">Computing Machinery and Intelligence</a>, which sought to discuss the all-important question, “can machines think?” Turing hoped to find an answer to the question of whether computers can demonstrate true intelligence. However, this question rather vague and hard to measure (“too meaningless to deserve discussion,” in Turing’s words). Instead, Turing proposes an alternate challenge: to create a machine that can perform at human levels or better in the “Imitation Game,” a Q&amp;A game where a computer is slated against a human interrogator who is tasked with distinguishing the computer from a human counterpart. Turing claims that “in about fifty years' time it will be possible to programme computers … to make them play the imitation game so well that an average interrogator will not have more than 70 per cent, chance of making the right identification.” Interestingly, Turing offers no suggestions for an implementation of such a machine in this paper. He simply discusses whether it’s possible, whether the question he poses is relevant, and what objections there might be to this discussion. The “Turing test” introduced in this paper would come to serve as a benchmark for future programs and their thinking power, or lack thereof.</p>
<p>Turing also notably made the distinction that only digital computers would be “allowed to take part” in the game, which at the time was non-obvious, and forecasts that machines might one day be able to compete with humans in activities like playing chess, which was realized in 1997 when the IBM computer Deep Blue famously beat reigning chess world champion Garry Kasparov. Another amusing note made in Turing’s paper was that this game might be “unfair” for the machine if humans were discovered to have telepathic abilities, which was apparently another open question at the time.</p>
</section>
<section id="notable-following-accomplishments-in-ai" class="level2">
<h2 class="anchored" data-anchor-id="notable-following-accomplishments-in-ai">Notable Following Accomplishments in AI</h2>
<p>Turing’s paper may have officially proposed the challenge to create artificial intelligence, but he never uses the term itself. This term was coined 6 years later, by John McCarthy, at the Dartmouth Summer Research Project on Artificial Intelligence, a two-month long conference that brought together some of the brightest minds in the field to study the topic. At this conference, Herbert Simon and Alan Newell presented their breakthrough program, the Logic Theorist, which was capable of proving mathematical theorems and represented the first prototype of a “thinking machine”. This program was inspired by the work of Alan Turing, Claude Shannon and Kurt Gödel, and modeled the thinking process by breaking mathematical expressions down into elements and sub-elements, transforming these symbols into proofs by building them back up in a hierarchical fashion.</p>
<p>Two years later, the Perceptron was introduced, a program written by Frank Rosenblatt which was capable of distinguishing punch cards with different characteristics from one another. Rosenblatt toted his accomplishment as “the first machine which is capable of having an original idea”. This is an overstatement, but the accomplishment was impressive nonetheless. The program was implemented on the massive IBM 704, and, most importantly, was the first instance of a neural network being trained for the purpose of computation. The concept of a neural network had already been <a href="https://www.historyofinformation.com/detail.php?entryid=782">introduced conceptually</a> in 1943, but Rosenblatt’s implementation was the first actual neural network that was trained, and laid the foundation for decades of neural networks research in decades to come.</p>
<p><img src="media/media/image2.jpeg" class="img-fluid"></p>
<p>Of course, between the Perceptron and the transformer there were decades of innovation in deep learning, but I wanted to mention these initial innovations as they seem to have paved the way for the “game” of AI, where researchers develop ever-more complex and intelligent models, striving to create a machine that can truly think. Since the introduction of the Perceptron, neural networks have been modified and designed for use in computing a wide variety of tasks and applications, with different architectures proving better suited to certain applications. I’ve written another post which provides an overview of how these networks work and some applications following the guidance of Andrej Karpathy’s course on neural networks; this is a good resource for background information. For this post, I will focus on one particular kind of neural network: the transformer. The transformer architecture has revolutionized the field of deep learning, proved useful in almost every application for neural networks since its introduction in 2017, and given rise to a slew of models capable of passing the Turing test.</p>
</section>
<section id="transformers-the-structure-and-implementation" class="level2">
<h2 class="anchored" data-anchor-id="transformers-the-structure-and-implementation">Transformers: The Structure and Implementation</h2>
<p>The transformer architecture was initially introduced for use in the field of machine translation , with the aim of providing more detailed context when translating long input sequences. Recurrent and convolutional neural networks, the two models prevalent at the time, suffered from an inability to retain long-term information, as they only could factor in information from the last state of the encoder. This means that as the distance between words in an input increased, the computational cost also increases, making it prohibitively expensive to accurately factor in distant words. The transformer address this limitation by “eschewing recurrence and instead relying entirely on the attention mechanism” (<a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Paper</a>).</p>
<p>The transformer architecture basically allows the neural network to gain increased awareness of other tokens in both the input and output using <strong>attention</strong>, a technique of assigning importance to other tokens so that more important elements can be given more weight in deciding the next output token. Attention can be thought of as a series of matrix transformations, designed so that the resulting values have been aligned to prioritize tokens that provide more context in the input sequence.</p>
<p><img src="media/media/image3.png" class="img-fluid"></p>
<p>This diagram from the Transformer paper introduces the basic structure of the architecture. On the left hand side, we see an encoder block (colored in grey), and on the right we have the decoder block. The paper suggests the use of 6 encoder and 6 decoder blocks, although these numbers are hyperparameters and can be tweaked according to the use case. Each encoder block consists of two main components; a <strong>multi-head attention</strong> layer, which I will elaborate on, and a fully connected feed-forward neural network, which is essentially a standard multi-layer perceptron. To allow for information from the input to each layer to carry over more freely, there are also <strong>residual connections</strong>, which skip the corresponding layer and are added to its output in a <strong>layer normalization</strong>.</p>
<p>In the decoder block, we have essentially the same setup, except that an additional masked multi-head attention layer is added, which allows the model to pay attention to only previous tokens while generating further output.</p>
<section id="multi-headed-self-attention" class="level4">
<h4 class="anchored" data-anchor-id="multi-headed-self-attention">Multi-Headed Self-Attention</h4>
<p>The self-attention mechanism basically calculates how important each word should be in the decision-making process of choosing the next one.</p>
<p><img src="media/media/image4.png" class="img-fluid"></p>
<p>This mechanism is implemented by a series of vector operations on the vector embeddings for each token. For each of the decoder or encoder’s input vectors (each of which represents a token), we create three additional vectors; a query, key and value vector, by multiplying each embedding vector from the input by three weight matrices that were trained during the training process. So we take input vector x1, multiply it by Wq, Wk and Wv (the weight matrices for queries, keys and values) to get q1, k1, and v1 (the query, key and value vector). To calculate the attention score of x1 with respect to all other inputs x2...xn we multiply the query vector q1 by the key vectors of the other inputs k1...kn. This score is scaled down proportionally to the dimension of the key vectors to reduce volatility, passed through the softmax function to normalize, and multiplied by the value vectors. If the softmax value resulting from the attention score is very low, the corresponding value vector will have small values, signaling that the corresponding token should not be given much weight. If the attention score was relatively large, then that value vector will be given more weight. The value vectors are then summed to produce the final output of self-attention for this one input. In the above example, we see that “the”, “animal”, “street” and “it” are given more attention than the other words in the sequence.</p>
<p>This was the attention mechanism for a single input token. In practice, this process is executed on an entire input sequence in parallel by constructing matrices from the input vectors and performing matrix operations to calculate the attention score. There are also multiple “heads” in multi-head attention, where each head is initialized with differing weight matrices, and thus “pays attention” to different characteristics in the input sequence. The diagram below from Jay Alammar’s <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> (a great resource if you’d like more details on the attention process) models this operation with 8 attention heads, where the output matrix is Z.</p>
<p><img src="media/media/image5.png" class="img-fluid"></p>
</section>
<section id="positional-encoding" class="level4">
<h4 class="anchored" data-anchor-id="positional-encoding">Positional Encoding</h4>
<p>We also need some way to encode the relative position of inputs in the input sequence; currently, the input “the black horse drank water from the well” would be assigned the same attention scores as “water well from horse the black” or another permutation of the same sequence. From the Transformer paper: “the network and the self-attention mechanism is permutation invariant.” Thus, we add another feature to the initial embedding; a positional encoding.</p>
<p><img src="media/media/image6.png" class="img-fluid"></p>
<p>The positional encoding is another vector which is added to the input embedding, and serves to add additional context to the output from the attention layer.</p>
</section>
<section id="final-output" class="level4">
<h4 class="anchored" data-anchor-id="final-output">Final Output</h4>
<p>After passing through each attention layer and feed-forward neural net in each encoder and decoder, the output from the final decoder block is passed to a linear layer, which transforms the decoder’s output to a vector of logits of dimension equivalent to the model’s vocabulary size. These logits can then be converted to log probabilities, and the final output is chosen based on the element with the highest probability. This process is repeated until the output has been fully generated.</p>
</section>
</section>
<section id="impact-and-applications" class="level2">
<h2 class="anchored" data-anchor-id="impact-and-applications">Impact and Applications</h2>
<p>The transformer architecture is relatively simple, yet its impacts have been broad. The underlying improvement is the attention mechanism; previously, language models would have trouble distinguishing important keywords in a long input sequence, which provides a significant roadblock to quality output. With the transformer, we see the development of chatbots that <a href="https://www.mlyearning.org/chatgpt-passes-turing-test/#:~:text=Still%2C%20ChatGPT%20was%20able%20to,the%20Turing%20test%20last%20summer.">pass the Turing Test</a>, solving the decades-old challenge, as well as neural networks that can <a href="https://openai.com/dall-e-2">generate and understand images</a>, solve complex scientific problems like <a href="https://www.nature.com/articles/s41586-021-03819-2">protein structure prediction</a>, and use computer vision to make progress toward <a href="https://www.tesla.com/AI">fully self-driving cars</a>. The transformer is unique in its versatility; before it was released, many of the models that are now trained using the transformer architecture used widely divergent architectures.</p>
<p>Besides the attention mechanism, the transformer’s most important characteristic is its computational efficiency, due to the parallel nature of the matrix operations inherent to its architecture. This has allowed transformer-based models to increase exponentially in size; in 2018, the first GPT released by OpenAI had 0.12 billion parameters —&nbsp;now, GPT-4 is estimated to have trillions of parameters. This is advantageous as it enable bigger and more powerful models, but also has lead to a new consideration: the environmental impact of training and using neural networks.</p>
<p>GPT-3, with a model size of 175 billion parameters, <a href="https://lambdalabs.com/blog/demystifying-gpt-3">has been estimated</a> to require 355 years of training time and $4.6 million in compute cost if it were trained on a single GPU. The associated carbon footprint is “equivalent to driving 112 gasoline powered cars for a year,” according to <a href="https://news.climate.columbia.edu/2023/06/09/ais-growing-carbon-footprint/">this blog post</a> from the Columbia Climate School. This is a staggering amount of computation; computation that is often powered by fossil fuels and also requires <a href="https://clivethompson.medium.com/ai-is-thirsty-37f99f24a26e#:~:text=And%20when%20you're%20chatting,and%20where%20ChatGPT%20is%20deployed.">huge amounts</a> of fresh water for cooling. <a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445922">Other papers</a> have discussed the dangers of training models on unfiltered and unregulated data, arguing that the size of the dataset doesn’t necessarily guarantee its diversity. So, while the transformer has revolutionized how neural networks are trained and helped to blow winds into the sails of deep learning research and public interest in artificial intelligence, it is not a perfect model, and should be modified so that environmental costs and the model’s biases are considered.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>