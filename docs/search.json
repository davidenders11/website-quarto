[
  {
    "objectID": "blog-sample/about.html",
    "href": "blog-sample/about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "blog-sample/posts-sample/welcome/index.html",
    "href": "blog-sample/posts-sample/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Writing",
    "section": "",
    "text": "The Game of AI: And How the Transformer Transformed It\n\n\n\n\n\n\n\ncomputer science\n\n\nneural networks\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2023\n\n\nDavid Enders\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction To Neural Networks\n\n\n\n\n\n\n\ncomputer science\n\n\nneural networks\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2023\n\n\nDavid Enders\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/transformer/index.html",
    "href": "posts/transformer/index.html",
    "title": "The Game of AI: And How the Transformer Transformed It",
    "section": "",
    "text": "For decades, the world has been fascinated with the concept of artificial intelligence; we have written stories about it, produced movies, written countless computer programs, and theorized about what is possible to achieve and what isn’t. This history is an impressive one, but also somewhat humorous to me; it all seems like a bit of a game, with teams of scientists and researchers competing to create a machine that can play chess the best, or draw the best pictures, or write more human-like stories."
  },
  {
    "objectID": "posts/transformer/index.html#the-turing-test-how-it-started",
    "href": "posts/transformer/index.html#the-turing-test-how-it-started",
    "title": "The Game of AI: And How the Transformer Transformed It",
    "section": "The Turing Test: How it Started",
    "text": "The Turing Test: How it Started\nIn 1950, Alan Turing published a paper titled Computing Machinery and Intelligence, which sought to discuss the all-important question, “can machines think?” Turing hoped to find an answer to the question of whether computers can demonstrate true intelligence. However, this question rather vague and hard to measure (“too meaningless to deserve discussion,” in Turing’s words). Instead, Turing proposes an alternate challenge: to create a machine that can perform at human levels or better in the “Imitation Game,” a Q&A game where a computer is slated against a human interrogator who is tasked with distinguishing the computer from a human counterpart. Turing claims that “in about fifty years' time it will be possible to programme computers … to make them play the imitation game so well that an average interrogator will not have more than 70 per cent, chance of making the right identification.” Interestingly, Turing offers no suggestions for an implementation of such a machine in this paper. He simply discusses whether it’s possible, whether the question he poses is relevant, and what objections there might be to this discussion. The “Turing test” introduced in this paper would come to serve as a benchmark for future programs and their thinking power, or lack thereof.\nTuring also notably made the distinction that only digital computers would be “allowed to take part” in the game, which at the time was non-obvious, and forecasts that machines might one day be able to compete with humans in activities like playing chess, which was realized in 1997 when the IBM computer Deep Blue famously beat reigning chess world champion Garry Kasparov. Another amusing note made in Turing’s paper was that this game might be “unfair” for the machine if humans were discovered to have telepathic abilities, which was apparently another open question at the time."
  },
  {
    "objectID": "posts/transformer/index.html#notable-following-accomplishments-in-ai",
    "href": "posts/transformer/index.html#notable-following-accomplishments-in-ai",
    "title": "The Game of AI: And How the Transformer Transformed It",
    "section": "Notable Following Accomplishments in AI",
    "text": "Notable Following Accomplishments in AI\nTuring’s paper may have officially proposed the challenge to create artificial intelligence, but he never uses the term itself. This term was coined 6 years later, by John McCarthy, at the Dartmouth Summer Research Project on Artificial Intelligence, a two-month long conference that brought together some of the brightest minds in the field to study the topic. At this conference, Herbert Simon and Alan Newell presented their breakthrough program, the Logic Theorist, which was capable of proving mathematical theorems and represented the first prototype of a “thinking machine”. This program was inspired by the work of Alan Turing, Claude Shannon and Kurt Gödel, and modeled the thinking process by breaking mathematical expressions down into elements and sub-elements, transforming these symbols into proofs by building them back up in a hierarchical fashion.\nTwo years later, the Perceptron was introduced, a program written by Frank Rosenblatt which was capable of distinguishing punch cards with different characteristics from one another. Rosenblatt toted his accomplishment as “the first machine which is capable of having an original idea”. This is an overstatement, but the accomplishment was impressive nonetheless. The program was implemented on the massive IBM 704, and, most importantly, was the first instance of a neural network being trained for the purpose of computation. The concept of a neural network had already been introduced conceptually in 1943, but Rosenblatt’s implementation was the first actual neural network that was trained, and laid the foundation for decades of neural networks research in decades to come.\n\nOf course, between the Perceptron and the transformer there were decades of innovation in deep learning, but I wanted to mention these initial innovations as they seem to have paved the way for the “game” of AI, where researchers develop ever-more complex and intelligent models, striving to create a machine that can truly think. Since the introduction of the Perceptron, neural networks have been modified and designed for use in computing a wide variety of tasks and applications, with different architectures proving better suited to certain applications. I’ve written another post which provides an overview of how these networks work and some applications following the guidance of Andrej Karpathy’s course on neural networks; this is a good resource for background information. For this post, I will focus on one particular kind of neural network: the transformer. The transformer architecture has revolutionized the field of deep learning, proved useful in almost every application for neural networks since its introduction in 2017, and given rise to a slew of models capable of passing the Turing test."
  },
  {
    "objectID": "posts/transformer/index.html#transformers-the-structure-and-implementation",
    "href": "posts/transformer/index.html#transformers-the-structure-and-implementation",
    "title": "The Game of AI: And How the Transformer Transformed It",
    "section": "Transformers: The Structure and Implementation",
    "text": "Transformers: The Structure and Implementation\nThe transformer architecture was initially introduced for use in the field of machine translation , with the aim of providing more detailed context when translating long input sequences. Recurrent and convolutional neural networks, the two models prevalent at the time, suffered from an inability to retain long-term information, as they only could factor in information from the last state of the encoder. This means that as the distance between words in an input increased, the computational cost also increases, making it prohibitively expensive to accurately factor in distant words. The transformer address this limitation by “eschewing recurrence and instead relying entirely on the attention mechanism” (Attention Paper).\nThe transformer architecture basically allows the neural network to gain increased awareness of other tokens in both the input and output using attention, a technique of assigning importance to other tokens so that more important elements can be given more weight in deciding the next output token. Attention can be thought of as a series of matrix transformations, designed so that the resulting values have been aligned to prioritize tokens that provide more context in the input sequence.\n\nThis diagram from the Transformer paper introduces the basic structure of the architecture. On the left hand side, we see an encoder block (colored in grey), and on the right we have the decoder block. The paper suggests the use of 6 encoder and 6 decoder blocks, although these numbers are hyperparameters and can be tweaked according to the use case. Each encoder block consists of two main components; a multi-head attention layer, which I will elaborate on, and a fully connected feed-forward neural network, which is essentially a standard multi-layer perceptron. To allow for information from the input to each layer to carry over more freely, there are also residual connections, which skip the corresponding layer and are added to its output in a layer normalization.\nIn the decoder block, we have essentially the same setup, except that an additional masked multi-head attention layer is added, which allows the model to pay attention to only previous tokens while generating further output.\n\nMulti-Headed Self-Attention\nThe self-attention mechanism basically calculates how important each word should be in the decision-making process of choosing the next one.\n\nThis mechanism is implemented by a series of vector operations on the vector embeddings for each token. For each of the decoder or encoder’s input vectors (each of which represents a token), we create three additional vectors; a query, key and value vector, by multiplying each embedding vector from the input by three weight matrices that were trained during the training process. So we take input vector x1, multiply it by Wq, Wk and Wv (the weight matrices for queries, keys and values) to get q1, k1, and v1 (the query, key and value vector). To calculate the attention score of x1 with respect to all other inputs x2...xn we multiply the query vector q1 by the key vectors of the other inputs k1...kn. This score is scaled down proportionally to the dimension of the key vectors to reduce volatility, passed through the softmax function to normalize, and multiplied by the value vectors. If the softmax value resulting from the attention score is very low, the corresponding value vector will have small values, signaling that the corresponding token should not be given much weight. If the attention score was relatively large, then that value vector will be given more weight. The value vectors are then summed to produce the final output of self-attention for this one input. In the above example, we see that “the”, “animal”, “street” and “it” are given more attention than the other words in the sequence.\nThis was the attention mechanism for a single input token. In practice, this process is executed on an entire input sequence in parallel by constructing matrices from the input vectors and performing matrix operations to calculate the attention score. There are also multiple “heads” in multi-head attention, where each head is initialized with differing weight matrices, and thus “pays attention” to different characteristics in the input sequence. The diagram below from Jay Alammar’s The Illustrated Transformer (a great resource if you’d like more details on the attention process) models this operation with 8 attention heads, where the output matrix is Z.\n\n\n\nPositional Encoding\nWe also need some way to encode the relative position of inputs in the input sequence; currently, the input “the black horse drank water from the well” would be assigned the same attention scores as “water well from horse the black” or another permutation of the same sequence. From the Transformer paper: “the network and the self-attention mechanism is permutation invariant.” Thus, we add another feature to the initial embedding; a positional encoding.\n\nThe positional encoding is another vector which is added to the input embedding, and serves to add additional context to the output from the attention layer.\n\n\nFinal Output\nAfter passing through each attention layer and feed-forward neural net in each encoder and decoder, the output from the final decoder block is passed to a linear layer, which transforms the decoder’s output to a vector of logits of dimension equivalent to the model’s vocabulary size. These logits can then be converted to log probabilities, and the final output is chosen based on the element with the highest probability. This process is repeated until the output has been fully generated."
  },
  {
    "objectID": "posts/transformer/index.html#impact-and-applications",
    "href": "posts/transformer/index.html#impact-and-applications",
    "title": "The Game of AI: And How the Transformer Transformed It",
    "section": "Impact and Applications",
    "text": "Impact and Applications\nThe transformer architecture is relatively simple, yet its impacts have been broad. The underlying improvement is the attention mechanism; previously, language models would have trouble distinguishing important keywords in a long input sequence, which provides a significant roadblock to quality output. With the transformer, we see the development of chatbots that pass the Turing Test, solving the decades-old challenge, as well as neural networks that can generate and understand images, solve complex scientific problems like protein structure prediction, and use computer vision to make progress toward fully self-driving cars. The transformer is unique in its versatility; before it was released, many of the models that are now trained using the transformer architecture used widely divergent architectures.\nBesides the attention mechanism, the transformer’s most important characteristic is its computational efficiency, due to the parallel nature of the matrix operations inherent to its architecture. This has allowed transformer-based models to increase exponentially in size; in 2018, the first GPT released by OpenAI had 0.12 billion parameters — now, GPT-4 is estimated to have trillions of parameters. This is advantageous as it enable bigger and more powerful models, but also has lead to a new consideration: the environmental impact of training and using neural networks.\nGPT-3, with a model size of 175 billion parameters, has been estimated to require 355 years of training time and $4.6 million in compute cost if it were trained on a single GPU. The associated carbon footprint is “equivalent to driving 112 gasoline powered cars for a year,” according to this blog post from the Columbia Climate School. This is a staggering amount of computation; computation that is often powered by fossil fuels and also requires huge amounts of fresh water for cooling. Other papers have discussed the dangers of training models on unfiltered and unregulated data, arguing that the size of the dataset doesn’t necessarily guarantee its diversity. So, while the transformer has revolutionized how neural networks are trained and helped to blow winds into the sails of deep learning research and public interest in artificial intelligence, it is not a perfect model, and should be modified so that environmental costs and the model’s biases are considered."
  },
  {
    "objectID": "posts/neural-nets-intro/index.html",
    "href": "posts/neural-nets-intro/index.html",
    "title": "Introduction To Neural Networks",
    "section": "",
    "text": "Becoming a neural networks “hero” is a lofty goal to achieve in 13 hours of lecture, but an enticing offer for someone looking to venture into the world of machine learning. After spending the last summer helping incorporate AI-powered tools into the workflow at the company I interned with, I was eager to learn more about the history of artificial intelligence and the implementation of neural networks. Andrej Karpathy, an influential AI researcher and one of the founding members of OpenAI, offers a free lecture series called Neural Networks: Zero to Hero, which I decided to study. While I’m certainly no hero yet, the course provided me with a strong foundational understanding of the core concepts that comprise neural networks, as well as meaningful practice implementing these in PyTorch. In this post, I intend to share my learning from the course, focusing on concepts and leaving out most of the math and code. The aim of this post is not to act as a substitute for Andrej’s incredibly detailed lectures, but to document the knowledge I’ve gained and serve as a primer for other beginners to the topic. If you have the time, I highly recommend working through the course (the total lecture time is about 13 hours, but it took me closer to 30 hours, accounting for time spent note-taking, coding and replaying difficult sections).\n\n\nBefore starting the lecture, I primed myself with some introductory research. This paper was especially helpful in providing context. Below are some of the basics:\nA neural network is any collection of neurons arranged in such a way that their interactions yields fruitful results. While neural networks aren’t specific to machine learning by definition (the human brain is the original neural network), the term is now most often associated with the artificial neural network. Artificial neural networks (ANN) were pioneered when Warren McCulloch and and Walter Pitts developed an algorithm to imitate human brain activity in the 1940s (more on the history of neural networks here).\n\nNeurons in an ANN are, in essence, mathematical equations. They take a collection of inputs, perform some operation on these inputs, and produce a single output, which can then be transmitted to other neurons. Neurons in an ANN are similar to nerve cells in many ways:\n\nboth receive many input signals, assign weights to these, and process them\nboth transmit a single output to receivers when enough input is present\ndata processing occurs on a local level (within each neuron)\n“memory” is distributed throughout the system (long-term memory is stored in synapse weights, short-term memory in impulses)\nboth networks' synapse connections are strengthened by experience\ncertain groups of neurons getting activated will cause other neurons to fire.\n\nIn an ANN, neurons are arranged into layers, where the collection of neurons receiving inputs are the input layer, the intermediary neurons that do most of the processing are the hidden layer, and the final layer producing a result is the output layer. Neural networks form conclusions and make decisions by assigning weights to each input, which determines how much impact each input has. Weights and biases are the parameters that, through trial and error, are adjusted during training until the network arrives at a correct model.\n\nSo, the defining characteristics of an ANN are its architecture (how neurons are arranged in layers and interconnected), the training algorithm (i.e. the technique for calculating and adjusting weights on connections), and its activation function (the function that determines whether a neuron should be activated or not based on the given inputs). The activation in the neurons of the output layer will determine the model’s “answer” to whatever question was posed. Activation functions often also normalize output to the proper desired range (i.e. 0 to 1).\n\n\n\nWith some basic terms defined, we can now examine how these networks are trained at a baseline level. I mentioned that neurons are basically mathematical expressions. In fact, their most important characteristic in the context of training a network is that any operation that takes place in a neuron must be differentiable. This allows us to calculate gradients at each neuron with respect to the network’s loss function. The loss function measures the overall performance of a neural network by comparing target outcomes and actual outcomes, so the goal is always to minimize the loss function. Gradients represent the impact that each parameter has on the loss function, and are calculated using local derivatives at each neuron and the chain rule. The gradient provides information that allows the network to adjust these parameters to decrease the network’s loss. Simply put, we use derivatives to measure what impact each neuron has on the output. If the impact aligns with our target outcome, we increase that neuron’s impact, otherwise we reduce that neuron’s impact (using weights and biases). This is how the model learns.\nTo get a gradient at each neuron in the network, we need some way to calculate the derivative of each parameter with respect to its impact on the loss function. Using a process called backpropagation, this process is relatively simple.\n\nTake this neuron from Andrej’s first lecture, which has two inputs (x1 and x2), two weights (w1 and w2), one bias b and the activation function tanh. The derivative of output o with respect to itself is 1. That's our base case. Now we can go back one step; do/dn is the derivative of tanh(n) at n = 0.8814, which is equal to 1-tanh(0.8814)^2 = 0.5. So we can fill in the gradients of o and n with 1 and 0.5, respectively (these are already filled in in the diagram above). The next four gradients are simple to calculate: since each of the four values are being added, the local derivative is 1 for each of them. We use the chain rule with the ongoing overall derivative, multiplying 0.5*1 in each case to get a gradient of 0.5. To get the leftmost gradients, we calculate local derivatives d(w2)/d(x2w2) = x2 = 0, d(x2)/d(x2w2) = w2 = 1, d(w1)/d(x1w1) = x1 = 2, and finally d(x1)/d(x1w1) = w1 = -3. Now we can get the gradient by multiplying the local derivative by the output’s gradient, so w2’s gradient is 0*0.5=0, x2’s is 1*0.5=0.5, w1’s gradient is 2*0.5=1 and x1’s is -3*0.5=-1.5.\nThere are many steps involved in calculating gradients and implementing backpropagation, but the arithmetic is simple. We begin with the rightmost local derivative, then we recursively iterate left, or backpropagate, multiplying each node’s local derivative by its output’s gradient. These gradients are the core element we need to train our neural network. This is an isolated example in one neuron, but we now know that if we need the final output o to increase, we can achieve this by making the weight w1 or the bias b more positive. These parameters can then be recursively adjusted until the output reaches the desired value. Backpropagation is the core mechanism that enables the training of neural networks. Using backpropagation, the parameters in a neural network can essentially be randomly initialized, and they will eventually converge to the correct values during model training.\n\n\n\nAs mentioned above, a neural network can be as straightforward as a single layer of neurons that modify inputs by applying some weights and some mathematical expression. To understand how these are implemented, Andrej uses an example problem, where the goal is to generate more names or name-like strings using a model that is trained on a long list of existing names. This problem can be solved using a neural network, but a neural network isn’t the easiest solution. We can also use a probability distribution matrix and a character-level bigram language model, where we store pairs of subsequent characters and their corresponding frequencies, then use these to calculate the probability that one letter will follow another. With this probability distribution, we can make reasonably accurate guesses about which letter should follow the current letter; for example, the letter z is highly unlikely to follow the letter q, so the language model would almost never pick z next when the current letter is q. Conversely, the letter a is a common starting letter for names, so the model should assign a high probability that it follows the “start” character . (which is also the end character in this implementation and the graph below).\nBelow is a graph of these frequencies from a large dataset of words used in the exercises. Each cell denotes the frequency that the second character follows the first character in the dataset. If we divide each cell by the sum of its row (the total number of times the first character appeared in the dataset), we have a probability distribution showing how likely it is that each of the second characters were to follow the first character. Using this probability distribution is one way to generate words. For each character, starting at the first, we use the probability distribution to “pick” the next character, so characters that appeared next to each other in the dataset are likely to appear next to each other in the output.\n\nThis is relatively sound theoretically, but you can imagine that choosing a character only based on the previous output isn’t a very effective method. This is also not a neural network – since we aren’t training the model, we’re simply using existing probabilities to guess what a good output might be. Here are some sample “names” the model generated in the second lecture:\nmor.\naxx.\nminaymoryles.\nkondlaisah.\nanchshizarie.\nThese names are indisputably not very name-like. However, in order to improve our output, we must be able to quantify its quality in some standard way. To do this, we’ll use the concept of likelihood; how likely an output is to be produced given a model’s probability distribution. So the likelihood of something like emma, which is in the dataset, should be reasonably high and something like xqxzi which certainly isn’t in the dataset, should be near-zero. The likelihood of the dataset is the product of all the individual probabilities that each element in the dataset could be generated. To make this more stable and easier to work with, instead of multiplying all of the probabilities, we sum the log of the individual probabilities, giving us the log likelihood. Since probabilities are between 0 and 1, making the logs of the probabilities negative, we negate their sum (the log likelihood) and use the negative log likelihood. To arrive at our final measure for output quality (the loss function), we divide the negative log likelihood by the number of values in the dataset and are left with an average negative log likelihood, which is a metric to describe how likely the model would be to reproduce the training set.\nWe can use this to create a neural network with the bigram language model. To train it, we follow a process similar to the one in the first example, iteratively backpropagating and modifying the weights in accordance with their gradients, thus minimizing the average negative log likelihood. To codify inputting a character into the model to calculate the next character, we use “one-hot” encodings of each character, where an integer is encoded as a matrix where each value is zeroed-out except the the value at the index corresponding to the integer’s value, which is 1. So the letter d which has index 3 would be represented as the matrix\n[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nEach element in this vector represents the position for one of the 27 characters of the alphabet, plus an extra for the start/stop character .. To create the foundation for training the model, we can create a randomly-initialized 27-by-27 matrix representing the weights: 27 neurons, each takes 27 inputs. So when we multiply the input matrix (1x27) by the weights matrix (27x27), we get a 1x27 output matrix. Each element in the output matrix represents the logit (log probability) for that character, and is typically “squished” using the softmax activation function, to represent a probability, in this case the probability of this character being chosen.\nAt the beginning, these outputted probabilities won’t be accurate or useful. However, all of the operations described above are differentiable, so we can follow a similar process as done in the first example to tune the weights. To train the network, we iteratively backpropagate and modify the weights in accordance with their gradients with respect to the loss function, thus minimizing the average negative log likelihood. Eventually, the weights matrix will converge to a more “correct” matrix that does a better job of assigning probabilities for the next character given an input matrix representing the current character.\nUnfortunately, since the bigram model only takes into account the previous character, even a neural network trained on lots of data will output mediocre results (in fact, once it’s fully trained, it will be an equivalent model to the earlier, counting-based probability model using frequencies). To improve these results, we must increase the size of context we are considering, by adding more layers to our neural network. The important takeaway is that we are able to start off with a completely random collection of weights, assign these weights to modify input characters, and get probability estimates for the next character. Then, simply by calculating the derivatives of each parameter with respect to the output’s accuracy, we were able to iteratively improve the weights until we had a model that performed as well as it could given only a single character as context. This example also introduces one of the central challenges to implementing a neural network: how best to represent the input in a numerical format that can be fed into the network as input. In this example, a vector with a one-hot encoding worked well, but this encoding may vary greatly from network to network.\n\n\n\nThe next step in creating a more intelligent model is to give it more knowledge or context. To build a multi-layer perceptron, we will expand on the bigram model by adding two more layers and slightly altering our method for representing inputs.\nTo provide more context to the model, we now define a block size, or how many previous characters are given to determine the next character (in “real” large language models, the block size would determine how many words or chunks of words are used to predict the next). Our training set is now the set of all substrings (of block_size length) of names in the list of words, and the label for each substring is the character following it. For simplicity, let's assume block_size = 3.\n\nAnother feature introduced in this section is an embeddings layer, where we define an embeddings lookup table that stores an embedding for each character, thus converting each input (the 3-character string) to an n-dimensional embedding. This is done to normalize inputs and codify their values in a standard way. The embeddings are then adjusted as the network trains. Our new network’s architecture is as follows: the embeddings layer outputs embeddings that squash the inputs into a pre-defined space. Then, this embedding is the input to the hidden layer, the size of which we can choose. A design choice like the number of neurons in the hidden layer is called a hyperparameter, and is left to the developer to define and later optimize to improve the model’s performance. The hidden layer must have the same number of outputs as each neuron in the output layer takes as inputs. Finally, the output layer will have 27 neurons, one for each character. The hidden layer and the output layer each have a matrix of weights, which are used to process the inputs and are modified to train the model. The logits in the output layer are then exponentiated and normalized to sum to 1, so they can be used as probabilities.\nThis model is ready to train. However, we now have over 10,000 parameters and our dataset has more than 30,000 values. Calculating the output, backpropagating through each parameter, and adjusting the weights for each parameter is very computationally expensive, and while feasible for a network of this size, is not scalable. Instead, we choose minibatches, or smaller subsets of the dataset, to train the model. So for each pass, we choose a random, fixed-size subset of the data, and use the loss function with respect to that data to tune the weights. The size of the minibatch is another hyperparameter; the tradeoff for the developer is the compute time per pass vs. the noise introduced by using incomplete datasets to train the model.\nWe must also choose a reasonable learning rate, which is the multiplier used to multiply the gradient to adjust weights. With a very small learning rate, the model will learn at an unwieldy, slow rate. With a very high learning rate, the loss will be unstable, as the parameters are being over-adjusted each pass through the network. To find a good learning rate, we can plot the learning rate against the loss, and find a rate at which the loss is decreasing consistently without much noise. In the graph below, the learning rate at 0.1 seems to be a good choice. Learning rate decay is another common practice, where the learning rate is decreased at a later point in training when the loss is already small, and the initial learning rate has become too high.\n\nAdditional concepts that are key to be cognizant of when training a neural network are overfitting and underfitting. Overfitting occurs when the neural network is trained too much on a certain dataset, and starts to essentially memorize that data. Then, when sampling from this neural network, it will too closely reproduce the data that it was fed instead of producing useful “new” data. Underfitting is the opposite, and takes place when the neural network hasn’t learned enough from the data. To mitigate and measure these phenomenons, we split our dataset into a training set (about 80% of the data), a development set (about 10% of the data), and a test set (the last 10% of the data). The training data is the only set that should be used to train the model. The development set should then be used to evaluate the network’s loss, since this data hasn’t been seen yet by the network. If the loss on the training data is much lower than the loss on the development data, the network is likely overfitted; it is performing much better on data it is familiar with. If the losses are approximately the same, the model is likely underfitted; it hasn’t learned enough, and the developer should likely tweak the hyperparameters (train longer, increase the size of the network, tweak the learning rate, etc.).\n\n\n\nThe model described above is functional and leaves us with relatively low loss, but there is some wasted effort inherent to its architecture, and other potential weaknesses which should be addressed. Since all of our weights are randomly initialized, the output probabilities will initially predict highly incorrect outcomes. The loss calculated on these initial values will be very high, much higher than the loss would be with evenly distributed probabilities (i.e. where each next character is equally likely to be predicted). This occurs when the weights are initialized with large random numbers, outputting very confident predictions, despite not having been trained to output confidently correct predictions. This can be mitigated by initializing the weights matrix for the output layer with values very close to 0, so that each logit in the output is near-zero, producing output probabilities that are near-equal. This simple step leaves us with a significantly lower initial loss, which then reduces total computation. Instead of wasting computation correcting the initial predictions, the model will approach accurate weights much faster. As the model learns, these near-zero weights will be adjusted to approximate their correct values.\nAnother improvement to the model is facilitated by batch normalization. This is another layer that is placed between hidden layers to scale inputs between layers, so that anomalies don’t take place. We’ve already discussed an embeddings layer which normalizes the inputs, but activations sent as inputs from one layer to the next are left raw. In batch normalization, these intermediate values are scaled and shifted using their standard deviation and mean. This addresses the issue of saturated neurons, where an activation function for a layer fails to output values evenly distributed along its range (i.e. evenly distributed between 0 and 1), but instead tends toward the extremes, creating hyperactive neurons and dead neurons that have no learning power at all (lots of 0s and lots of 1s). To normalize a batch, we center the pre-activation values, or the values that feed into the activation function, using the mean and the standard deviation of each batch to scale and shift the activations from the previous layer. The goal is to create a roughly Gaussian shape for the distribution of data inputted to the next layer.\nEven with these stabilizing measures, the multi-layer perceptron described is not very deep, and thus has limited learning capabilities. One factor in the current implementation that makes it difficult to create deeper networks is our method for processing inputs; currently, we are squashing all of our input into the neural network at the input layer simultaneously. This mutes the data and doesn’t allow for each piece of input to influence the outcome as much as it can. Another improvement to the model is the concept of the WaveNet, where smaller chunks of input data are concatenated, and these chunks are combined piece-by-piece.\n\nSuch a model enables an even deeper neural network, where each input element can have a greater impact. These improvements represent only a subset of optimizations to neural networks; this is its own field of research and requires many more dedicated posts to fully discuss. I will focus the rest of this post on lecture 7 (the most viewed lecture in the series) and the transformer architecture introduced there, which has revolutionized neural network implementation.\n\n\n\nThe transformer architecture was first introduced in the 2017 paper Attention Is All You Need, and forms the foundation for famous natural language models like ChatGPT (GPT = generative pre-trained transformer), as well as a long list of models beyond natural language processing. The basic idea is that training data should be able to communicate with other training data and decide which data to value more. “Attention” is the idea that the tokens are aware of each other’s existence and can focus on the more important ones. In this way, information can be gathered in a data-dependent way, where some pieces of training data are more valuable than others.\nWith the transformer architecture, small chunks, or batches, of the dataset are separated and used for training piece-by-piece, with a fixed batch size. The training data in language models is commonly referred to as a token, which is typically a chunk of characters smaller than an average word (a token in the name-prediction example would be a single character). Previously, all of our context information would inform the following prediction with equal weight or value: any character in the series of three characters in our context blocks was equally important. Now, with attention, a token can express affinities for other tokens, where a certain past token might factor in more heavily than others when calculating a prediction. This replicates real-world sentence formulation more accurately. For example, when choosing the correct verb in a sentence, the subject of the sentence should be a more important factor than the article the sentence started with. An attention head is where the the weighted sums representing relationships between tokens are calculated. Usually, multiple attention heads are computed in parallel, allowing the model to encode different types of relationships between tokens. It is important to note, however, that tokens within a batch can only communicate with themselves; there is no communication between batches of data.\n\nThe transformer paper suggests an architecture that stacks “attention layers” with multilayer perceptrons. In each attention layer, attention vectors are used to represent contextual relationships between tokens in a batch. Multiple attention vectors are used in multi-head attention layers, which are then combined using a weighted average. This setup creates a deeper neural network, which is capable of fitting our data much more closely, effectively learning more and incorporating context in a more powerful way.\n\n\n\nBy the end of lecture 7, the viewer has coded alongside Andrej to build a simplified model of a GPT which is capable of generating a specified number of tokens in a manner similar to the data it was trained on (i.e. generating Shakespeare-like content given Shakespeare’s work to train on). The hyperparameters are entirely configurable, so the developer is able to choose the learning rate, training iterations, number of embeddings, number of layers, the number of heads, head size, the batch size, the block size, and the dropout rate. When choosing these hyperparameters, I was tempted to push the size of the network ever higher – the bigger the better, right? Near the end of the video, Andrej mentions that the example network he trained took fifteen minutes to train on his high-powered GPU, and warns that anyone unequipped with a dedicated GPU shouldn’t attempt to train that network. I tried it anyway, but gave up when my MacBook grew alarmingly warm after six hours of CPU time and the training was still only halfway done.\nThis was quite interesting to me, as I had heard of a GPU before, but never thought to look into their use or the difference between a GPU and a CPU. Graphics processing units were originally developed to assist in rendering image or video, or performing other operations which require highly parallel mathematical/graph operations. GPUs have thousand of cores, which make them especially adept at performing lots of small operations in parallel, which is exactly what training a neural network requires. CPUs (core processing units) have between two and sixty-four cores, and are far more generalist than GPUs, primarily performing operations serially. For this reason, training a neural network using a CPU is exceptionally slow and prohibitively expensive depending on the size of the network. Considering the massive datasets and network sizes used to train production-ready neural networks such as ChatGPT, it’s easy to imagine how massive computation costs are for participating companies, and how much attention is given to training techniques and choosing well-suited hardware.\n\n\n\nLastly, an important note: training a very deep neural network with lots of language data will not produce a highly-capable AI assistant like ChatGPT without more work. The process described above and discussed throughout the lecture series is known as “pre-training”, where a model is fed massive amounts of language data and trained to generate more, similar data. However, a model that spits out random generated data that approximates its training data isn’t very useful. The model must be “fine-tuned” to match its intended purpose. A chat-model must be able to answer questions and converse, a code generator must be able to generate code based on a prompt, and so on. OpenAI released the diagram below explaining their fine-tuning process in this blog post introducing ChatGPT. The fine-tuning process requires far more human intervention, including ranking outputs, creating a reward model which rewards high-quality responses, and the utilization of reinforcement learning. This process is nuanced, and is not nearly as open-source as the pre-training stage is. This is where the neural network is converted from a raw neural network to a highly commercial product that can be sold and monetized, so logically these techniques are kept secret.\n\n\n\n\nWe’ve come a long way from backpropagation to a full-fledged natural language model with configurable hyperparameters, layer normalization, skip connections, self-attention, and multi-layer perceptrons. If this introduction piqued your interest, following the lecture series is the best way to expand on more difficult concepts and create your own implementation of a neural network. I will likely continue to write about neural networks, and am taking a class on the topic this coming semester, so stay tuned for more posts on the topic! Thanks for taking the time to read."
  },
  {
    "objectID": "posts/neural-nets-intro/index.html#defining-key-terms",
    "href": "posts/neural-nets-intro/index.html#defining-key-terms",
    "title": "Introduction To Neural Networks",
    "section": "",
    "text": "Before starting the lecture, I primed myself with some introductory research. This paper was especially helpful in providing context. Below are some of the basics:\nA neural network is any collection of neurons arranged in such a way that their interactions yields fruitful results. While neural networks aren’t specific to machine learning by definition (the human brain is the original neural network), the term is now most often associated with the artificial neural network. Artificial neural networks (ANN) were pioneered when Warren McCulloch and and Walter Pitts developed an algorithm to imitate human brain activity in the 1940s (more on the history of neural networks here).\n\nNeurons in an ANN are, in essence, mathematical equations. They take a collection of inputs, perform some operation on these inputs, and produce a single output, which can then be transmitted to other neurons. Neurons in an ANN are similar to nerve cells in many ways:\n\nboth receive many input signals, assign weights to these, and process them\nboth transmit a single output to receivers when enough input is present\ndata processing occurs on a local level (within each neuron)\n“memory” is distributed throughout the system (long-term memory is stored in synapse weights, short-term memory in impulses)\nboth networks' synapse connections are strengthened by experience\ncertain groups of neurons getting activated will cause other neurons to fire.\n\nIn an ANN, neurons are arranged into layers, where the collection of neurons receiving inputs are the input layer, the intermediary neurons that do most of the processing are the hidden layer, and the final layer producing a result is the output layer. Neural networks form conclusions and make decisions by assigning weights to each input, which determines how much impact each input has. Weights and biases are the parameters that, through trial and error, are adjusted during training until the network arrives at a correct model.\n\nSo, the defining characteristics of an ANN are its architecture (how neurons are arranged in layers and interconnected), the training algorithm (i.e. the technique for calculating and adjusting weights on connections), and its activation function (the function that determines whether a neuron should be activated or not based on the given inputs). The activation in the neurons of the output layer will determine the model’s “answer” to whatever question was posed. Activation functions often also normalize output to the proper desired range (i.e. 0 to 1)."
  },
  {
    "objectID": "posts/neural-nets-intro/index.html#backpropagation-as-a-training-method",
    "href": "posts/neural-nets-intro/index.html#backpropagation-as-a-training-method",
    "title": "Introduction To Neural Networks",
    "section": "",
    "text": "With some basic terms defined, we can now examine how these networks are trained at a baseline level. I mentioned that neurons are basically mathematical expressions. In fact, their most important characteristic in the context of training a network is that any operation that takes place in a neuron must be differentiable. This allows us to calculate gradients at each neuron with respect to the network’s loss function. The loss function measures the overall performance of a neural network by comparing target outcomes and actual outcomes, so the goal is always to minimize the loss function. Gradients represent the impact that each parameter has on the loss function, and are calculated using local derivatives at each neuron and the chain rule. The gradient provides information that allows the network to adjust these parameters to decrease the network’s loss. Simply put, we use derivatives to measure what impact each neuron has on the output. If the impact aligns with our target outcome, we increase that neuron’s impact, otherwise we reduce that neuron’s impact (using weights and biases). This is how the model learns.\nTo get a gradient at each neuron in the network, we need some way to calculate the derivative of each parameter with respect to its impact on the loss function. Using a process called backpropagation, this process is relatively simple.\n\nTake this neuron from Andrej’s first lecture, which has two inputs (x1 and x2), two weights (w1 and w2), one bias b and the activation function tanh. The derivative of output o with respect to itself is 1. That's our base case. Now we can go back one step; do/dn is the derivative of tanh(n) at n = 0.8814, which is equal to 1-tanh(0.8814)^2 = 0.5. So we can fill in the gradients of o and n with 1 and 0.5, respectively (these are already filled in in the diagram above). The next four gradients are simple to calculate: since each of the four values are being added, the local derivative is 1 for each of them. We use the chain rule with the ongoing overall derivative, multiplying 0.5*1 in each case to get a gradient of 0.5. To get the leftmost gradients, we calculate local derivatives d(w2)/d(x2w2) = x2 = 0, d(x2)/d(x2w2) = w2 = 1, d(w1)/d(x1w1) = x1 = 2, and finally d(x1)/d(x1w1) = w1 = -3. Now we can get the gradient by multiplying the local derivative by the output’s gradient, so w2’s gradient is 0*0.5=0, x2’s is 1*0.5=0.5, w1’s gradient is 2*0.5=1 and x1’s is -3*0.5=-1.5.\nThere are many steps involved in calculating gradients and implementing backpropagation, but the arithmetic is simple. We begin with the rightmost local derivative, then we recursively iterate left, or backpropagate, multiplying each node’s local derivative by its output’s gradient. These gradients are the core element we need to train our neural network. This is an isolated example in one neuron, but we now know that if we need the final output o to increase, we can achieve this by making the weight w1 or the bias b more positive. These parameters can then be recursively adjusted until the output reaches the desired value. Backpropagation is the core mechanism that enables the training of neural networks. Using backpropagation, the parameters in a neural network can essentially be randomly initialized, and they will eventually converge to the correct values during model training."
  },
  {
    "objectID": "posts/neural-nets-intro/index.html#a-single-layer-neural-network",
    "href": "posts/neural-nets-intro/index.html#a-single-layer-neural-network",
    "title": "Introduction To Neural Networks",
    "section": "",
    "text": "As mentioned above, a neural network can be as straightforward as a single layer of neurons that modify inputs by applying some weights and some mathematical expression. To understand how these are implemented, Andrej uses an example problem, where the goal is to generate more names or name-like strings using a model that is trained on a long list of existing names. This problem can be solved using a neural network, but a neural network isn’t the easiest solution. We can also use a probability distribution matrix and a character-level bigram language model, where we store pairs of subsequent characters and their corresponding frequencies, then use these to calculate the probability that one letter will follow another. With this probability distribution, we can make reasonably accurate guesses about which letter should follow the current letter; for example, the letter z is highly unlikely to follow the letter q, so the language model would almost never pick z next when the current letter is q. Conversely, the letter a is a common starting letter for names, so the model should assign a high probability that it follows the “start” character . (which is also the end character in this implementation and the graph below).\nBelow is a graph of these frequencies from a large dataset of words used in the exercises. Each cell denotes the frequency that the second character follows the first character in the dataset. If we divide each cell by the sum of its row (the total number of times the first character appeared in the dataset), we have a probability distribution showing how likely it is that each of the second characters were to follow the first character. Using this probability distribution is one way to generate words. For each character, starting at the first, we use the probability distribution to “pick” the next character, so characters that appeared next to each other in the dataset are likely to appear next to each other in the output.\n\nThis is relatively sound theoretically, but you can imagine that choosing a character only based on the previous output isn’t a very effective method. This is also not a neural network – since we aren’t training the model, we’re simply using existing probabilities to guess what a good output might be. Here are some sample “names” the model generated in the second lecture:\nmor.\naxx.\nminaymoryles.\nkondlaisah.\nanchshizarie.\nThese names are indisputably not very name-like. However, in order to improve our output, we must be able to quantify its quality in some standard way. To do this, we’ll use the concept of likelihood; how likely an output is to be produced given a model’s probability distribution. So the likelihood of something like emma, which is in the dataset, should be reasonably high and something like xqxzi which certainly isn’t in the dataset, should be near-zero. The likelihood of the dataset is the product of all the individual probabilities that each element in the dataset could be generated. To make this more stable and easier to work with, instead of multiplying all of the probabilities, we sum the log of the individual probabilities, giving us the log likelihood. Since probabilities are between 0 and 1, making the logs of the probabilities negative, we negate their sum (the log likelihood) and use the negative log likelihood. To arrive at our final measure for output quality (the loss function), we divide the negative log likelihood by the number of values in the dataset and are left with an average negative log likelihood, which is a metric to describe how likely the model would be to reproduce the training set.\nWe can use this to create a neural network with the bigram language model. To train it, we follow a process similar to the one in the first example, iteratively backpropagating and modifying the weights in accordance with their gradients, thus minimizing the average negative log likelihood. To codify inputting a character into the model to calculate the next character, we use “one-hot” encodings of each character, where an integer is encoded as a matrix where each value is zeroed-out except the the value at the index corresponding to the integer’s value, which is 1. So the letter d which has index 3 would be represented as the matrix\n[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nEach element in this vector represents the position for one of the 27 characters of the alphabet, plus an extra for the start/stop character .. To create the foundation for training the model, we can create a randomly-initialized 27-by-27 matrix representing the weights: 27 neurons, each takes 27 inputs. So when we multiply the input matrix (1x27) by the weights matrix (27x27), we get a 1x27 output matrix. Each element in the output matrix represents the logit (log probability) for that character, and is typically “squished” using the softmax activation function, to represent a probability, in this case the probability of this character being chosen.\nAt the beginning, these outputted probabilities won’t be accurate or useful. However, all of the operations described above are differentiable, so we can follow a similar process as done in the first example to tune the weights. To train the network, we iteratively backpropagate and modify the weights in accordance with their gradients with respect to the loss function, thus minimizing the average negative log likelihood. Eventually, the weights matrix will converge to a more “correct” matrix that does a better job of assigning probabilities for the next character given an input matrix representing the current character.\nUnfortunately, since the bigram model only takes into account the previous character, even a neural network trained on lots of data will output mediocre results (in fact, once it’s fully trained, it will be an equivalent model to the earlier, counting-based probability model using frequencies). To improve these results, we must increase the size of context we are considering, by adding more layers to our neural network. The important takeaway is that we are able to start off with a completely random collection of weights, assign these weights to modify input characters, and get probability estimates for the next character. Then, simply by calculating the derivatives of each parameter with respect to the output’s accuracy, we were able to iteratively improve the weights until we had a model that performed as well as it could given only a single character as context. This example also introduces one of the central challenges to implementing a neural network: how best to represent the input in a numerical format that can be fed into the network as input. In this example, a vector with a one-hot encoding worked well, but this encoding may vary greatly from network to network."
  },
  {
    "objectID": "posts/neural-nets-intro/index.html#building-a-multi-layer-perceptron",
    "href": "posts/neural-nets-intro/index.html#building-a-multi-layer-perceptron",
    "title": "Introduction To Neural Networks",
    "section": "",
    "text": "The next step in creating a more intelligent model is to give it more knowledge or context. To build a multi-layer perceptron, we will expand on the bigram model by adding two more layers and slightly altering our method for representing inputs.\nTo provide more context to the model, we now define a block size, or how many previous characters are given to determine the next character (in “real” large language models, the block size would determine how many words or chunks of words are used to predict the next). Our training set is now the set of all substrings (of block_size length) of names in the list of words, and the label for each substring is the character following it. For simplicity, let's assume block_size = 3.\n\nAnother feature introduced in this section is an embeddings layer, where we define an embeddings lookup table that stores an embedding for each character, thus converting each input (the 3-character string) to an n-dimensional embedding. This is done to normalize inputs and codify their values in a standard way. The embeddings are then adjusted as the network trains. Our new network’s architecture is as follows: the embeddings layer outputs embeddings that squash the inputs into a pre-defined space. Then, this embedding is the input to the hidden layer, the size of which we can choose. A design choice like the number of neurons in the hidden layer is called a hyperparameter, and is left to the developer to define and later optimize to improve the model’s performance. The hidden layer must have the same number of outputs as each neuron in the output layer takes as inputs. Finally, the output layer will have 27 neurons, one for each character. The hidden layer and the output layer each have a matrix of weights, which are used to process the inputs and are modified to train the model. The logits in the output layer are then exponentiated and normalized to sum to 1, so they can be used as probabilities.\nThis model is ready to train. However, we now have over 10,000 parameters and our dataset has more than 30,000 values. Calculating the output, backpropagating through each parameter, and adjusting the weights for each parameter is very computationally expensive, and while feasible for a network of this size, is not scalable. Instead, we choose minibatches, or smaller subsets of the dataset, to train the model. So for each pass, we choose a random, fixed-size subset of the data, and use the loss function with respect to that data to tune the weights. The size of the minibatch is another hyperparameter; the tradeoff for the developer is the compute time per pass vs. the noise introduced by using incomplete datasets to train the model.\nWe must also choose a reasonable learning rate, which is the multiplier used to multiply the gradient to adjust weights. With a very small learning rate, the model will learn at an unwieldy, slow rate. With a very high learning rate, the loss will be unstable, as the parameters are being over-adjusted each pass through the network. To find a good learning rate, we can plot the learning rate against the loss, and find a rate at which the loss is decreasing consistently without much noise. In the graph below, the learning rate at 0.1 seems to be a good choice. Learning rate decay is another common practice, where the learning rate is decreased at a later point in training when the loss is already small, and the initial learning rate has become too high.\n\nAdditional concepts that are key to be cognizant of when training a neural network are overfitting and underfitting. Overfitting occurs when the neural network is trained too much on a certain dataset, and starts to essentially memorize that data. Then, when sampling from this neural network, it will too closely reproduce the data that it was fed instead of producing useful “new” data. Underfitting is the opposite, and takes place when the neural network hasn’t learned enough from the data. To mitigate and measure these phenomenons, we split our dataset into a training set (about 80% of the data), a development set (about 10% of the data), and a test set (the last 10% of the data). The training data is the only set that should be used to train the model. The development set should then be used to evaluate the network’s loss, since this data hasn’t been seen yet by the network. If the loss on the training data is much lower than the loss on the development data, the network is likely overfitted; it is performing much better on data it is familiar with. If the losses are approximately the same, the model is likely underfitted; it hasn’t learned enough, and the developer should likely tweak the hyperparameters (train longer, increase the size of the network, tweak the learning rate, etc.)."
  },
  {
    "objectID": "posts/neural-nets-intro/index.html#improving-and-stabilizing-the-multilayer-perceptron",
    "href": "posts/neural-nets-intro/index.html#improving-and-stabilizing-the-multilayer-perceptron",
    "title": "Introduction To Neural Networks",
    "section": "",
    "text": "The model described above is functional and leaves us with relatively low loss, but there is some wasted effort inherent to its architecture, and other potential weaknesses which should be addressed. Since all of our weights are randomly initialized, the output probabilities will initially predict highly incorrect outcomes. The loss calculated on these initial values will be very high, much higher than the loss would be with evenly distributed probabilities (i.e. where each next character is equally likely to be predicted). This occurs when the weights are initialized with large random numbers, outputting very confident predictions, despite not having been trained to output confidently correct predictions. This can be mitigated by initializing the weights matrix for the output layer with values very close to 0, so that each logit in the output is near-zero, producing output probabilities that are near-equal. This simple step leaves us with a significantly lower initial loss, which then reduces total computation. Instead of wasting computation correcting the initial predictions, the model will approach accurate weights much faster. As the model learns, these near-zero weights will be adjusted to approximate their correct values.\nAnother improvement to the model is facilitated by batch normalization. This is another layer that is placed between hidden layers to scale inputs between layers, so that anomalies don’t take place. We’ve already discussed an embeddings layer which normalizes the inputs, but activations sent as inputs from one layer to the next are left raw. In batch normalization, these intermediate values are scaled and shifted using their standard deviation and mean. This addresses the issue of saturated neurons, where an activation function for a layer fails to output values evenly distributed along its range (i.e. evenly distributed between 0 and 1), but instead tends toward the extremes, creating hyperactive neurons and dead neurons that have no learning power at all (lots of 0s and lots of 1s). To normalize a batch, we center the pre-activation values, or the values that feed into the activation function, using the mean and the standard deviation of each batch to scale and shift the activations from the previous layer. The goal is to create a roughly Gaussian shape for the distribution of data inputted to the next layer.\nEven with these stabilizing measures, the multi-layer perceptron described is not very deep, and thus has limited learning capabilities. One factor in the current implementation that makes it difficult to create deeper networks is our method for processing inputs; currently, we are squashing all of our input into the neural network at the input layer simultaneously. This mutes the data and doesn’t allow for each piece of input to influence the outcome as much as it can. Another improvement to the model is the concept of the WaveNet, where smaller chunks of input data are concatenated, and these chunks are combined piece-by-piece.\n\nSuch a model enables an even deeper neural network, where each input element can have a greater impact. These improvements represent only a subset of optimizations to neural networks; this is its own field of research and requires many more dedicated posts to fully discuss. I will focus the rest of this post on lecture 7 (the most viewed lecture in the series) and the transformer architecture introduced there, which has revolutionized neural network implementation."
  },
  {
    "objectID": "posts/neural-nets-intro/index.html#the-transformer",
    "href": "posts/neural-nets-intro/index.html#the-transformer",
    "title": "Introduction To Neural Networks",
    "section": "",
    "text": "The transformer architecture was first introduced in the 2017 paper Attention Is All You Need, and forms the foundation for famous natural language models like ChatGPT (GPT = generative pre-trained transformer), as well as a long list of models beyond natural language processing. The basic idea is that training data should be able to communicate with other training data and decide which data to value more. “Attention” is the idea that the tokens are aware of each other’s existence and can focus on the more important ones. In this way, information can be gathered in a data-dependent way, where some pieces of training data are more valuable than others.\nWith the transformer architecture, small chunks, or batches, of the dataset are separated and used for training piece-by-piece, with a fixed batch size. The training data in language models is commonly referred to as a token, which is typically a chunk of characters smaller than an average word (a token in the name-prediction example would be a single character). Previously, all of our context information would inform the following prediction with equal weight or value: any character in the series of three characters in our context blocks was equally important. Now, with attention, a token can express affinities for other tokens, where a certain past token might factor in more heavily than others when calculating a prediction. This replicates real-world sentence formulation more accurately. For example, when choosing the correct verb in a sentence, the subject of the sentence should be a more important factor than the article the sentence started with. An attention head is where the the weighted sums representing relationships between tokens are calculated. Usually, multiple attention heads are computed in parallel, allowing the model to encode different types of relationships between tokens. It is important to note, however, that tokens within a batch can only communicate with themselves; there is no communication between batches of data.\n\nThe transformer paper suggests an architecture that stacks “attention layers” with multilayer perceptrons. In each attention layer, attention vectors are used to represent contextual relationships between tokens in a batch. Multiple attention vectors are used in multi-head attention layers, which are then combined using a weighted average. This setup creates a deeper neural network, which is capable of fitting our data much more closely, effectively learning more and incorporating context in a more powerful way."
  },
  {
    "objectID": "posts/neural-nets-intro/index.html#training-neural-networks",
    "href": "posts/neural-nets-intro/index.html#training-neural-networks",
    "title": "Introduction To Neural Networks",
    "section": "",
    "text": "By the end of lecture 7, the viewer has coded alongside Andrej to build a simplified model of a GPT which is capable of generating a specified number of tokens in a manner similar to the data it was trained on (i.e. generating Shakespeare-like content given Shakespeare’s work to train on). The hyperparameters are entirely configurable, so the developer is able to choose the learning rate, training iterations, number of embeddings, number of layers, the number of heads, head size, the batch size, the block size, and the dropout rate. When choosing these hyperparameters, I was tempted to push the size of the network ever higher – the bigger the better, right? Near the end of the video, Andrej mentions that the example network he trained took fifteen minutes to train on his high-powered GPU, and warns that anyone unequipped with a dedicated GPU shouldn’t attempt to train that network. I tried it anyway, but gave up when my MacBook grew alarmingly warm after six hours of CPU time and the training was still only halfway done.\nThis was quite interesting to me, as I had heard of a GPU before, but never thought to look into their use or the difference between a GPU and a CPU. Graphics processing units were originally developed to assist in rendering image or video, or performing other operations which require highly parallel mathematical/graph operations. GPUs have thousand of cores, which make them especially adept at performing lots of small operations in parallel, which is exactly what training a neural network requires. CPUs (core processing units) have between two and sixty-four cores, and are far more generalist than GPUs, primarily performing operations serially. For this reason, training a neural network using a CPU is exceptionally slow and prohibitively expensive depending on the size of the network. Considering the massive datasets and network sizes used to train production-ready neural networks such as ChatGPT, it’s easy to imagine how massive computation costs are for participating companies, and how much attention is given to training techniques and choosing well-suited hardware."
  },
  {
    "objectID": "posts/neural-nets-intro/index.html#pre-training-vs.-fine-tuning",
    "href": "posts/neural-nets-intro/index.html#pre-training-vs.-fine-tuning",
    "title": "Introduction To Neural Networks",
    "section": "",
    "text": "Lastly, an important note: training a very deep neural network with lots of language data will not produce a highly-capable AI assistant like ChatGPT without more work. The process described above and discussed throughout the lecture series is known as “pre-training”, where a model is fed massive amounts of language data and trained to generate more, similar data. However, a model that spits out random generated data that approximates its training data isn’t very useful. The model must be “fine-tuned” to match its intended purpose. A chat-model must be able to answer questions and converse, a code generator must be able to generate code based on a prompt, and so on. OpenAI released the diagram below explaining their fine-tuning process in this blog post introducing ChatGPT. The fine-tuning process requires far more human intervention, including ranking outputs, creating a reward model which rewards high-quality responses, and the utilization of reinforcement learning. This process is nuanced, and is not nearly as open-source as the pre-training stage is. This is where the neural network is converted from a raw neural network to a highly commercial product that can be sold and monetized, so logically these techniques are kept secret."
  },
  {
    "objectID": "posts/neural-nets-intro/index.html#wrapping-up",
    "href": "posts/neural-nets-intro/index.html#wrapping-up",
    "title": "Introduction To Neural Networks",
    "section": "",
    "text": "We’ve come a long way from backpropagation to a full-fledged natural language model with configurable hyperparameters, layer normalization, skip connections, self-attention, and multi-layer perceptrons. If this introduction piqued your interest, following the lecture series is the best way to expand on more difficult concepts and create your own implementation of a neural network. I will likely continue to write about neural networks, and am taking a class on the topic this coming semester, so stay tuned for more posts on the topic! Thanks for taking the time to read."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "David Enders",
    "section": "",
    "text": "I’m a fourth year student at Claremont McKenna College, majoring in computer science at Harvey Mudd College. I love to build and learn new things, and am currently recruiting for software engineering roles.\nI have work experience in embedded software engineering in the automotive space and full-stack web development, and have informal or academic experience in machine learning, compiler and programming language design, building AI-powered tools, and more. I also am participating in neural networks research, using generative models for climate prediction purposes.\nSome other interests of mine are woodworking and welding, listening to and making music, board sports (snowboard, surf, skate), and travelling.\nEmail: david.georg.enders@gmail.com"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "David Enders",
    "section": "",
    "text": "I’m a fourth year student at Claremont McKenna College, majoring in computer science at Harvey Mudd College. I love to build and learn new things, and am currently recruiting for software engineering roles.\nI have work experience in embedded software engineering in the automotive space and full-stack web development, and have informal or academic experience in machine learning, compiler and programming language design, building AI-powered tools, and more. I also am participating in neural networks research, using generative models for climate prediction purposes.\nSome other interests of mine are woodworking and welding, listening to and making music, board sports (snowboard, surf, skate), and travelling.\nEmail: david.georg.enders@gmail.com"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "David Enders",
    "section": "Projects",
    "text": "Projects\n\nAll-Hands Meeting Recap Generator\n\n\n\nVariational Autoencoder Implementation\n\n\nGmail Tasks Automation\n\n\nWordle Game and Bot"
  },
  {
    "objectID": "blog-sample/posts-sample/post-with-code/index.html",
    "href": "blog-sample/posts-sample/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "blog-sample/index.html",
    "href": "blog-sample/index.html",
    "title": "blog",
    "section": "",
    "text": "No matching items"
  }
]